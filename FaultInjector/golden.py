from Kernel import complete, gpu_kernel_execution, scheduler
from PyOpenTCU import Tensor
from log.logger import logger
from Schedulers import *


import numpy as np
import os
import sys


def create_matrix(size=[120, 120], scale=20, offset=10):
    return (np.random.rand(size[0], size[1]) * scale) - offset

def save_matrix(a, filename):
    path = os.getcwd()
    golden_path = os.path.join(path, "golden", filename)
    np.save(golden_path, a)


def tiling(a, b, c):
    tensor = Tensor()
    d = scheduler(a, b, c, tensor)
    save_matrix(d, "d")
    return d


def read_matrix(filename):
    path = os.getcwd()
    golden_path = os.path.join(path, "golden", filename + ".npy")
    return np.load(golden_path)

#def validate(d_golden, d, error_range):
    for row, columns in enumerate(d_golden):
        for column, golden in enumerate(columns):
            if abs( float(golden) - float(d[row][column])) > error_range:
                logger.warning(
                    f"Error: [{row}, {column}] does not have the correct value"
                )
                logger.warning(f"Expected {golden} obtained {d[row][column]}")
                return False    
    return True

def CTA_allocation_and_scheduler_vaidation(scheduler,a,b,c,d_golden):
    a = complete(a)
    b = complete(b)
    c = complete(c)

    tensor = Tensor()
    CTAs = scheduler.scheduler_algorithm(a,b,c)
    d_ = gpu_kernel_execution(a,b,c,CTAs,tensor)
    #validation = validate(d_golden, d_, 0.01)#low validation error since the computation is performed on the same numeric reppresentation float16
    #if( not(validation)):
    #    print("Validation Test  Failed for scheduling policy : " + str(scheduler.read_name()))
    #    sys.exit()

def golden(A_x_dim, A_B_common_dim, B_y_dim, min_value = 0, max_value = 1):
    #matrix generation & validation
    a = create_matrix(size=[A_x_dim,A_B_common_dim],scale= (max_value - min_value), offset= -min_value)
    b = create_matrix(size=[A_B_common_dim, B_y_dim],scale= (max_value - min_value), offset= -min_value)
    c = create_matrix(size=[A_x_dim, B_y_dim],scale= (max_value - min_value), offset= -min_value)
    d_ = tiling(a, b, c)
    d = np.matmul(a, b) + c

    #validation = validate(d, d_, 0.5* abs(max_value-min_value))#big error range for validation due to computation error float16/float32
    #if( not(validation)):
    #    print("Validation Test Failed")
    #    sys.exit()

    save_matrix(a,"a.npy")
    save_matrix(b,"b.npy")
    save_matrix(c,"c.npy")
    save_matrix(d_,"d.npy")

    #Scheduler CTAs allocation & validation
    TwoLevelRoundRobin_scheduler = TwoLevelRoundRobin()
    GlobalRoundRobin_scheduler = GlobalRoundRobin()
    Greedy_scheduler = Greedy()
    DistributedCTA_scheduler = DistributedCTA()
    DistributedBlock_scheduler = DistributedBlock()

    CTA_allocation_and_scheduler_vaidation(TwoLevelRoundRobin_scheduler,a,b,c,d_)#since SFPY uses float16 is reccomended that from now on the golden values are not those  generated from np.matmul since an instrisic error generated by using float32 should not be taken into consideration during fault injection 
    CTA_allocation_and_scheduler_vaidation(GlobalRoundRobin_scheduler,a,b,c,d_)
    CTA_allocation_and_scheduler_vaidation(Greedy_scheduler,a,b,c,d_)
    CTA_allocation_and_scheduler_vaidation(DistributedCTA_scheduler,a,b,c,d_)
    CTA_allocation_and_scheduler_vaidation(DistributedBlock_scheduler,a,b,c,d_)

    logger.warning("Golden value module completed \n")